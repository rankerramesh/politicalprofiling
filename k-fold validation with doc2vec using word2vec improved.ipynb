{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from datetime import datetime\n",
    "\n",
    "directory = os.fsencode(r\"C:\\Users\\dell\\Desktop\\py\\tweets\")\n",
    "#directoryPath=\"C:\\\\Users\\\\dell\\\\Desktop\\\\py\\\\tweets\\\\\";\n",
    "directoryPath=\"C:\\\\Users\\\\dell\\\\Desktop\\\\research\\\\nlp\\\\political figures tweets\\\\devnagari converted\\\\congress\\\\\"\n",
    "tweets_collection=[]\n",
    "#files = []\n",
    "# r=root, d=directories, f = files\n",
    "#congress\n",
    "for r, d, f in os.walk(directoryPath):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            fileContent=''\n",
    "            with open(os.path.join(r, file), encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    fileContent=fileContent+line\n",
    "                tweets_collection.append([fileContent,'congress'])\n",
    "\n",
    "#madhesbadi  \n",
    "directoryPath=\"C:\\\\Users\\\\dell\\\\Desktop\\\\research\\\\nlp\\\\political figures tweets\\\\devnagari converted\\\\madhesbadi\\\\\"\n",
    "for r, d, f in os.walk(directoryPath):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            fileContent=''\n",
    "            with open(os.path.join(r, file), encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    fileContent=fileContent+line\n",
    "                tweets_collection.append([fileContent,'madhesbadi'])\n",
    "         \n",
    "\n",
    "#mao      \n",
    "'''directoryPath=\"C:\\\\Users\\\\dell\\\\Desktop\\\\research\\\\nlp\\\\political figures tweets\\\\devnagari converted\\\\mao\\\\\"\n",
    "for r, d, f in os.walk(directoryPath):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            fileContent=''\n",
    "            with open(os.path.join(r, file), encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    fileContent=fileContent+line\n",
    "                tweets_collection.append([fileContent,'mao'])\n",
    "'''    \n",
    "      \n",
    "#nonp  \n",
    "directoryPath=\"C:\\\\Users\\\\dell\\\\Desktop\\\\research\\\\nlp\\\\political figures tweets\\\\devnagari converted\\\\nonpolitical\\\\\"\n",
    "for r, d, f in os.walk(directoryPath):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            fileContent=''\n",
    "            with open(os.path.join(r, file), encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    fileContent=fileContent+line\n",
    "                tweets_collection.append([fileContent,'nonpolitical'])\n",
    "\n",
    "            \n",
    "#raprapa   \n",
    "directoryPath=\"C:\\\\Users\\\\dell\\\\Desktop\\\\research\\\\nlp\\\\political figures tweets\\\\devnagari converted\\\\raprapa\\\\\"\n",
    "for r, d, f in os.walk(directoryPath):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            fileContent=''\n",
    "            with open(os.path.join(r, file), encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    fileContent=fileContent+line\n",
    "                tweets_collection.append([fileContent,'raprapa'])\n",
    "\n",
    "\n",
    "            \n",
    "#sajha   \n",
    "directoryPath=\"C:\\\\Users\\\\dell\\\\Desktop\\\\research\\\\nlp\\\\political figures tweets\\\\devnagari converted\\\\sajha\\\\\"\n",
    "for r, d, f in os.walk(directoryPath):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            fileContent=''\n",
    "            with open(os.path.join(r, file), encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    fileContent=fileContent+line\n",
    "                tweets_collection.append([fileContent,'sajha'])\n",
    "\n",
    "\n",
    "#uml       \n",
    "directoryPath=\"C:\\\\Users\\\\dell\\\\Desktop\\\\research\\\\nlp\\\\political figures tweets\\\\devnagari converted\\\\uml\\\\\"\n",
    "for r, d, f in os.walk(directoryPath):\n",
    "    for file in f:\n",
    "        if '.csv' in file:\n",
    "            fileContent=''\n",
    "            with open(os.path.join(r, file), encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    fileContent=fileContent+line\n",
    "                tweets_collection.append([fileContent,'uml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = array(tweets_collection)\n",
    "kfold = KFold(10, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 17:22:52\n"
     ]
    }
   ],
   "source": [
    "#resultfile = open(r\"C:\\Users\\dell\\Desktop\\ressulttextd2v15.txt\",\"a\") \n",
    "for train, test in kfold.split(tweet_data):\n",
    "    congressStr = ''\n",
    "    umlStr = ''\n",
    "    #maoistStr =''\n",
    "    madhesiStr = ''\n",
    "    sajhaStr = ''\n",
    "    raprapaStr = ''\n",
    "    nonPliticalStr = ''\n",
    "    \n",
    "    for item in tweet_data[train]:\n",
    "         if item[1]=='congress':\n",
    "            congressStr=congressStr + item[0]\n",
    "         elif item[1]=='madhesbadi':\n",
    "            madhesiStr=madhesiStr+item[0]\n",
    "         #elif item[1]=='mao':\n",
    "            #maoistStr=maoistStr+item[0]\n",
    "         elif item[1]=='nonpolitical':\n",
    "            nonPliticalStr=nonPliticalStr+item[0]\n",
    "         elif item[1]=='raprapa':\n",
    "            raprapaStr=raprapaStr+item[0]\n",
    "         elif item[1]=='sajha':\n",
    "            sajhaStr=sajhaStr+item[0]\n",
    "         elif item[1]=='uml':\n",
    "            umlStr=umlStr+item[0]\n",
    "            \n",
    "    congressWords = congressStr.splitlines()\n",
    "    congressdata = [] \n",
    "    # iterate through each tweets of congress\n",
    "    for i in congressWords: \n",
    "        congressdata.append(i.split())\n",
    "   \n",
    "    madhesiWords = madhesiStr.splitlines()\n",
    "    madhesidata = [] \n",
    "    for i in madhesiWords: \n",
    "        madhesidata.append(i.split()) \n",
    "    '''\n",
    "    maoWords = maoistStr.splitlines()\n",
    "    maodata = [] \n",
    "    for i in maoWords: \n",
    "        maodata.append(i.split()) \n",
    "    '''\n",
    "    nonpWords = nonPliticalStr.splitlines()\n",
    "    nonpdata = [] \n",
    "    for i in nonpWords: \n",
    "        nonpdata.append(i.split()) \n",
    "        \n",
    "    raprapaWords = raprapaStr.splitlines()\n",
    "    raprapadata = [] \n",
    "    for i in raprapaWords: \n",
    "        raprapadata.append(i.split()) \n",
    "\n",
    "    sajhaWords = sajhaStr.splitlines()\n",
    "    sajhadata = [] \n",
    "    for i in sajhaWords: \n",
    "        sajhadata.append(i.split()) \n",
    "\n",
    "    umlWords = umlStr.splitlines()\n",
    "    umldata = [] \n",
    "    for i in umlWords: \n",
    "        umldata.append(i.split()) \n",
    "        \n",
    "    # Create CBOW model \n",
    "    #path = get_tmpfile(\"word2vec.model\")\n",
    "    CBOW_model_congress = gensim.models.Word2Vec(congressdata, min_count = 2,  \n",
    "                              size = 300, window = 10,workers=2)#, sg=1) \n",
    "    #CBOW_model_congress.save(\"word2vec.model\")\n",
    "    \n",
    "    CBOW_model_uml = gensim.models.Word2Vec(umldata, min_count = 2,  \n",
    "                              size = 300, window = 10,workers=2)#,sg=1) \n",
    "    #CBOW_model_uml.save(\"word2vec.model\")\n",
    "\n",
    "\n",
    "    #CBOW_model_mao = gensim.models.Word2Vec(maodata, min_count = 2,  \n",
    "    #                          size = 50, window = 5,workers=2)#,sg=1) \n",
    "    #CBOW_model_mao.save(\"word2vec.model\")\n",
    "    \n",
    "    CBOW_model_madhesi = gensim.models.Word2Vec(madhesidata, min_count = 2,  \n",
    "                              size = 300, window = 10,workers=2)#,sg=1) \n",
    "    #CBOW_model_madhesi.save(\"word2vec.model\")\n",
    "\n",
    "    CBOW_model_sajha = gensim.models.Word2Vec(sajhadata, min_count = 2,  \n",
    "                              size = 300, window =10,workers=2)#,sg=1) \n",
    "    #CBOW_model_sajha.save(\"word2vec.model\")\n",
    "    \n",
    "    CBOW_model_raprapa = gensim.models.Word2Vec(raprapadata, min_count = 2,  \n",
    "                              size = 300, window = 10,workers=2)#,sg=1) \n",
    "    #CBOW_model_raprapa.save(\"word2vec.model\")\n",
    "    \n",
    "    CBOW_model_nonp = gensim.models.Word2Vec(nonpdata, min_count = 2,  \n",
    "                              size = 300, window = 10,workers=2)#,sg=1) \n",
    "    #CBOW_model_nonp.save(\"word2vec.model\")\n",
    "    #model = gensim.models.Word2Vec.load(\"word2vec.model\")\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "  \n",
    "    for testItem in tweet_data[test]:\n",
    "        individualString=testItem[0]\n",
    "        individualWords = individualString.splitlines()\n",
    "        individualdata = []\n",
    "        \n",
    "        # iterate through each sentence in the file \n",
    "        for i in individualWords: \n",
    "            individualdata.append(i.split())\n",
    "\n",
    "        CBOW_model_individual = gensim.models.Word2Vec(individualdata, min_count = 2,  \n",
    "                                      size = 300, window = 10,workers=2)#,sg=1) \n",
    "        #CBOW_model_individual.save(\"word2vec1.model\") \n",
    "        \n",
    "        dimensionsArray = np.zeros((6,300),dtype=np.longdouble)\n",
    "        conWCount=0\n",
    "        umlWCount=0\n",
    "        #maoWCount=0\n",
    "        madhWCount=0\n",
    "        sajhaWCount=0\n",
    "        rapraWCount=0\n",
    "        nonWCount=0\n",
    "        for aWord in CBOW_model_individual.wv.vocab:\n",
    "            if aWord in CBOW_model_congress.wv.vocab:\n",
    "                conWCount=conWCount+1\n",
    "                word_vector=CBOW_model_congress.wv.word_vec(aWord)\n",
    "                dimCounter=0\n",
    "                for dim in word_vector:\n",
    "                    dimensionsArray[0][dimCounter]=dimensionsArray[0][dimCounter]+dim\n",
    "                    dimCounter=dimCounter+1\n",
    "                    if dimCounter==299:\n",
    "                        break\n",
    "\n",
    "        for aWord in CBOW_model_individual.wv.vocab:\n",
    "            if aWord in CBOW_model_uml.wv.vocab:\n",
    "                umlWCount=umlWCount+1\n",
    "                word_vector=CBOW_model_uml.wv.word_vec(aWord)\n",
    "                dimCounter=0\n",
    "                for dim in word_vector:\n",
    "                    dimensionsArray[1][dimCounter]=dimensionsArray[1][dimCounter]+dim\n",
    "                    dimCounter=dimCounter+1\n",
    "                    if dimCounter==299:\n",
    "                        break\n",
    "        '''\n",
    "        for aWord in CBOW_model_individual.wv.vocab:\n",
    "            if aWord in CBOW_model_mao.wv.vocab:\n",
    "                maoWCount=maoWCount+1\n",
    "                word_vector=CBOW_model_mao.wv.word_vec(aWord)\n",
    "                dimCounter=0\n",
    "                for dim in word_vector:\n",
    "                    dimensionsArray[2][dimCounter]=dimensionsArray[2][dimCounter]+dim\n",
    "                    dimCounter=dimCounter+1\n",
    "                    if dimCounter==49:\n",
    "                        break\n",
    "        '''\n",
    "        for aWord in CBOW_model_individual.wv.vocab:\n",
    "            if aWord in CBOW_model_madhesi.wv.vocab:\n",
    "                madhWCount=madhWCount+1\n",
    "                word_vector=CBOW_model_madhesi.wv.word_vec(aWord)\n",
    "                dimCounter=0\n",
    "                for dim in word_vector:\n",
    "                    dimensionsArray[2][dimCounter]=dimensionsArray[2][dimCounter]+dim\n",
    "                    dimCounter=dimCounter+1\n",
    "                    if dimCounter==299:\n",
    "                        break\n",
    "\n",
    "        for aWord in CBOW_model_individual.wv.vocab:\n",
    "            if aWord in CBOW_model_sajha.wv.vocab:\n",
    "                sajhaWCount=sajhaWCount+1\n",
    "                word_vector=CBOW_model_sajha.wv.word_vec(aWord)\n",
    "                dimCounter=0\n",
    "                for dim in word_vector:\n",
    "                    dimensionsArray[3][dimCounter]=dimensionsArray[3][dimCounter]+dim\n",
    "                    dimCounter=dimCounter+1\n",
    "                    if dimCounter==299:\n",
    "                        break\n",
    "\n",
    "        for aWord in CBOW_model_individual.wv.vocab:\n",
    "            if aWord in CBOW_model_raprapa.wv.vocab:\n",
    "                rapraWCount=rapraWCount+1\n",
    "                word_vector=CBOW_model_raprapa.wv.word_vec(aWord)\n",
    "                dimCounter=0\n",
    "                for dim in word_vector:\n",
    "                    dimensionsArray[4][dimCounter]=dimensionsArray[4][dimCounter]+dim\n",
    "                    dimCounter=dimCounter+1\n",
    "                    if dimCounter==299:\n",
    "                        break\n",
    "\n",
    "        for aWord in CBOW_model_individual.wv.vocab:\n",
    "            if aWord in CBOW_model_nonp.wv.vocab:\n",
    "                nonWCount=nonWCount+1\n",
    "                word_vector=CBOW_model_nonp.wv.word_vec(aWord)\n",
    "                dimCounter=0\n",
    "                for dim in word_vector:\n",
    "                    dimensionsArray[5][dimCounter]=dimensionsArray[5][dimCounter]+dim\n",
    "                    dimCounter=dimCounter+1\n",
    "                    if dimCounter==299:\n",
    "                        break\n",
    "        \n",
    "        dimensionsArray[0][:] = [x / conWCount for x in dimensionsArray[0]]\n",
    "        dimensionsArray[1][:] = [x / umlWCount for x in dimensionsArray[1]]\n",
    "        #dimensionsArray[2][:] = [x / maoWCount for x in dimensionsArray[2]]\n",
    "        dimensionsArray[2][:] = [x / madhWCount for x in dimensionsArray[2]]\n",
    "        dimensionsArray[3][:] = [x / sajhaWCount for x in dimensionsArray[3]]\n",
    "        dimensionsArray[4][:] = [x / rapraWCount for x in dimensionsArray[4]]\n",
    "        dimensionsArray[5][:] = [x / nonWCount for x in dimensionsArray[5]]\n",
    "        \n",
    "        dfAll=pd.DataFrame(dimensionsArray)  \n",
    "    \n",
    "        \n",
    "        indivisualDimensionsArray = np.zeros((1,300),dtype=np.longdouble)\n",
    "        for aWord in CBOW_model_individual.wv.vocab:\n",
    "            word_vector=CBOW_model_individual.wv.word_vec(aWord)\n",
    "            dimCounter=0\n",
    "            for dim in word_vector:\n",
    "                indivisualDimensionsArray[0][dimCounter]=indivisualDimensionsArray[0][dimCounter]+dim\n",
    "                dimCounter=dimCounter+1\n",
    "                if dimCounter==299:\n",
    "                    break\n",
    "        dfPersonA=pd.DataFrame(indivisualDimensionsArray)\n",
    "        resultfile.writelines(\"\\n\"+testItem[1]+\"\\n\") \n",
    "        for simi in cosine_similarity(dfAll, dfPersonA):\n",
    "            resultfile.writelines(str(simi[0]) +\"\\n\")\n",
    "        break    \n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    resultfile.writelines(\"\\n\") \n",
    "    resultfile.writelines(\"\\n\") \n",
    "    break\n",
    "resultfile.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14801884,  0.17013658, -0.0943301 , ...,  0.04762081,\n",
       "        -0.3237455 ,  0.        ],\n",
       "       [ 0.14691746,  0.28224322,  0.01402679, ...,  0.05902386,\n",
       "        -0.21432162,  0.        ],\n",
       "       [ 0.04258833,  0.17742235,  0.05189126, ...,  0.07656067,\n",
       "        -0.29571226,  0.        ],\n",
       "       [ 0.17958795,  0.26892216,  0.1746833 , ..., -0.11412652,\n",
       "        -0.32581979,  0.        ],\n",
       "       [ 0.04626197, -0.03916102, -0.06258884, ...,  0.09059965,\n",
       "        -0.27342925,  0.        ],\n",
       "       [ 0.09219553,  0.18397406,  0.0486013 , ..., -0.00784407,\n",
       "        -0.1292016 ,  0.        ]], dtype=float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensionsArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
